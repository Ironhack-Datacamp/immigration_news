{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"abc_news_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publish_date       0\n",
       "headline_text     12\n",
       "financial_year     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['publish_date'] = pd.to_datetime(news['publish_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publish_date      datetime64[ns]\n",
       "headline_text             object\n",
       "financial_year            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "      <th>financial_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>aba decid against commun broadcast licence</td>\n",
       "      <td>2002-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>act fire wit must be aware of defam</td>\n",
       "      <td>2002-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>a g call for infrastructure protect summit</td>\n",
       "      <td>2002-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>2002-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-02-19</td>\n",
       "      <td>air nz strike to affect australian travel</td>\n",
       "      <td>2002-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186013</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>vision of flame approach corryong in victoria</td>\n",
       "      <td>2019-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186014</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>wa police and govern backflip on drug amnesti bin</td>\n",
       "      <td>2019-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186015</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>we have fear for their safeti victorian premier</td>\n",
       "      <td>2019-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186016</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>when do the start</td>\n",
       "      <td>2019-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186017</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>yarraville shoot woman dead man critic injur</td>\n",
       "      <td>2019-07-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1186018 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        publish_date                                      headline_text  \\\n",
       "0         2003-02-19         aba decid against commun broadcast licence   \n",
       "1         2003-02-19                act fire wit must be aware of defam   \n",
       "2         2003-02-19         a g call for infrastructure protect summit   \n",
       "3         2003-02-19           air nz staff in aust strike for pay rise   \n",
       "4         2003-02-19          air nz strike to affect australian travel   \n",
       "...              ...                                                ...   \n",
       "1186013   2019-12-31      vision of flame approach corryong in victoria   \n",
       "1186014   2019-12-31  wa police and govern backflip on drug amnesti bin   \n",
       "1186015   2019-12-31    we have fear for their safeti victorian premier   \n",
       "1186016   2019-12-31                                  when do the start   \n",
       "1186017   2019-12-31       yarraville shoot woman dead man critic injur   \n",
       "\n",
       "        financial_year  \n",
       "0           2002-07-01  \n",
       "1           2002-07-01  \n",
       "2           2002-07-01  \n",
       "3           2002-07-01  \n",
       "4           2002-07-01  \n",
       "...                ...  \n",
       "1186013     2019-07-01  \n",
       "1186014     2019-07-01  \n",
       "1186015     2019-07-01  \n",
       "1186016     2019-07-01  \n",
       "1186017     2019-07-01  \n",
       "\n",
       "[1186018 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_copy = news.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#broke computer, do only with subsetting\n",
    "#cv = CountVectorizer(stop_words='english')\n",
    "#data_cv = cv.fit_transform(news_copy.headline_text)\n",
    "#news_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "#news_dtm.index = news_copy.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make subsets of df to get top 15 words per financial year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2012-07-01    93574\n",
       "2013-07-01    86497\n",
       "2011-07-01    81546\n",
       "2007-07-01    79743\n",
       "2008-07-01    78062\n",
       "2010-07-01    76406\n",
       "2014-07-01    76302\n",
       "2009-07-01    75475\n",
       "2005-07-01    73499\n",
       "2015-07-01    73027\n",
       "2004-07-01    72856\n",
       "2003-07-01    72753\n",
       "2006-07-01    67690\n",
       "2016-07-01    53234\n",
       "2017-07-01    44629\n",
       "2018-07-01    36344\n",
       "2002-07-01    27436\n",
       "2019-07-01    16945\n",
       "Name: financial_year, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#overall decrease in number of news headings\n",
    "#may need rate of immigration articles as % of total for the year?\n",
    "news_copy['financial_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2002-07-01', '2003-07-01', '2004-07-01', '2005-07-01',\n",
       "       '2006-07-01', '2007-07-01', '2008-07-01', '2009-07-01',\n",
       "       '2010-07-01', '2011-07-01', '2012-07-01', '2013-07-01',\n",
       "       '2014-07-01', '2015-07-01', '2016-07-01', '2017-07-01',\n",
       "       '2018-07-01', '2019-07-01'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_copy['financial_year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2018_19 = news_copy.loc[(news_copy['financial_year'] == '2018-19')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-652931b9da37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset2018_19\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheadline_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnews_dtm_subset2018_19\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnews_dtm_subset2018_19\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubset2018_19\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/matrix/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/matrix/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1127\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m   1130\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(subset2018_19.headline_text)\n",
    "news_dtm_subset2018_19 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "news_dtm_subset2018_19.index = subset2018_19.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dtm_subset2018_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dtm_subset2018_19.sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_dtm_subset2018_19.sum().sort_values(ascending=False).head(15)\n",
    "police        994\n",
    "says          906\n",
    "australia     882\n",
    "new           878\n",
    "trump         855\n",
    "australian    853\n",
    "man           849\n",
    "election      810\n",
    "sydney        605\n",
    "nsw           522\n",
    "court         499\n",
    "federal       490\n",
    "donald        467\n",
    "death         463\n",
    "government    451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2003_04 = news_copy.loc[(news_copy['financial_year'] == '2003-04')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(subset2003_04.headline_text)\n",
    "news_dtm_subset2003_04 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "news_dtm_subset2003_04.index = subset2003_04.index\n",
    "news_dtm_subset2003_04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_dtm_subset2003_04.sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_dtm_subset2003_04.sum().sort_values(ascending=False).head(15)\n",
    "police     2765\n",
    "new        2142\n",
    "govt       2111\n",
    "man        1728\n",
    "council    1498\n",
    "plan       1359\n",
    "iraq       1269\n",
    "court      1237\n",
    "says       1165\n",
    "nsw         913\n",
    "killed      873\n",
    "report      843\n",
    "wa          842\n",
    "claims      818\n",
    "qld         817"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2004_05 = news_copy.loc[(news_copy['financial_year'] == '2004-05')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(subset2004_05.headline_text)\n",
    "news_dtm_subset2004_05 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "news_dtm_subset2004_05.index = subset2004_05.index\n",
    "news_dtm_subset2004_05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_dtm_subset2004_05.sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#news_dtm_subset2004_05.sum().sort_values(ascending=False).head(15)\n",
    "police     2875\n",
    "govt       2161\n",
    "new        1991\n",
    "council    1602\n",
    "man        1565\n",
    "plan       1389\n",
    "says       1176\n",
    "urged      1024\n",
    "court      1021\n",
    "iraq        948\n",
    "water       822\n",
    "claims      804\n",
    "health      800\n",
    "boost       763\n",
    "tsunami     762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2005_06 = news_copy.loc[(news_copy['financial_year'] == '2005-06')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(subset2005_06.headline_text)\n",
    "news_dtm_subset2005_06 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "news_dtm_subset2005_06.index = subset2005_06.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_dtm_subset2005_06.sum().sort_values(ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_dtm_subset2005_06.sum().sort_values(ascending=False).head(15)\n",
    "\n",
    "police     2778\n",
    "govt       2484\n",
    "new        2038\n",
    "man        1589\n",
    "council    1528\n",
    "says       1287\n",
    "plan       1261\n",
    "urged      1085\n",
    "court      1070\n",
    "water       920\n",
    "health      913\n",
    "crash       901\n",
    "death       821\n",
    "group       724\n",
    "pm          717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2006_07 = news_copy.loc[(news_copy['financial_year'] == '2006-07')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(subset2006_07.headline_text)\n",
    "news_dtm_subset2006_07 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "news_dtm_subset2006_07.index = subset2006_07.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_dtm_subset2006_07.sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_dtm_subset2006_07.sum().sort_values(ascending=False).head(15)\n",
    "\n",
    "police     2759\n",
    "govt       2501\n",
    "water      1755\n",
    "new        1745\n",
    "man        1641\n",
    "says       1554\n",
    "council    1414\n",
    "plan       1145\n",
    "court       981\n",
    "pm          934\n",
    "crash       910\n",
    "qld         842\n",
    "urged       790\n",
    "nsw         743\n",
    "wa          700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2007_08 = news_copy.loc[(news_copy['financial_year'] == '2007-08')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(subset2007_08.headline_text)\n",
    "news_dtm_subset2007_08 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "news_dtm_subset2007_08.index = subset2007_08.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_dtm_subset2007_08.sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_dtm_subset2007_08.sum().sort_values(ascending=False).head(15)\n",
    "police     3308\n",
    "govt       2440\n",
    "man        2167\n",
    "new        2045\n",
    "court      1294\n",
    "council    1272\n",
    "says       1212\n",
    "qld        1151\n",
    "water      1075\n",
    "death       933\n",
    "charged     926\n",
    "plan        907\n",
    "crash       895\n",
    "nsw         873\n",
    "health      871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2008_09 = news_copy.loc[(news_copy['financial_year'] == '2008-09')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(subset2008_09.headline_text)\n",
    "news_dtm_subset2008_09 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "news_dtm_subset2008_09.index = subset2008_09.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_dtm_subset2008_09.sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#news_dtm_subset2008_09.sum().sort_values(ascending=False).head(15)\n",
    "police     3006\n",
    "man        2363\n",
    "govt       2220\n",
    "new        1887\n",
    "qld        1313\n",
    "says       1225\n",
    "court      1211\n",
    "council    1059\n",
    "crash       991\n",
    "charged     928\n",
    "water       925\n",
    "death       884\n",
    "plan        830\n",
    "sa          780\n",
    "killed      758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2009_10 = news_copy.loc[(news_copy['financial_year'] == '2009-10')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(subset2009_10.headline_text)\n",
    "news_dtm_subset2009_10 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "news_dtm_subset2009_10.index = subset2009_10.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_dtm_subset2009_10.sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_dtm_subset2009_10.sum().sort_values(ascending=False).head(15)\n",
    "interview    3077\n",
    "police       2376\n",
    "man          2045\n",
    "new          1647\n",
    "court        1165\n",
    "council      1124\n",
    "water         916\n",
    "accused       902\n",
    "crash         901\n",
    "death         835\n",
    "health        808\n",
    "says          798\n",
    "charged       784\n",
    "murder        780\n",
    "plan          777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2015_16 = news_copy.loc[(news_copy['financial_year'] == '2015-16')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(subset2015_16.headline_text)\n",
    "news_dtm_subset2015_16 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "news_dtm_subset2015_16.index = subset2015_16.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_dtm_subset2015_16.sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_dtm_subset2015_16.sum().sort_values(ascending=False).head(15)\n",
    "new           2372\n",
    "man           2136\n",
    "police        2043\n",
    "says          1954\n",
    "australia     1553\n",
    "australian    1420\n",
    "wa            1406\n",
    "nsw           1243\n",
    "government    1095\n",
    "rural         1074\n",
    "court         1047\n",
    "sydney        1002\n",
    "queensland     986\n",
    "sa             936\n",
    "world          929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2016_17 = news_copy.loc[(news_copy['financial_year'] == '2016-17')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(subset2016_17.headline_text)\n",
    "news_dtm_subset2016_17 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "news_dtm_subset2016_17.index = subset2016_17.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_dtm_subset2016_17.sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_dtm_subset2016_17.sum().sort_values(ascending=False).head(15)\n",
    "says          1606\n",
    "new           1486\n",
    "police        1461\n",
    "australia     1448\n",
    "man           1360\n",
    "trump         1330\n",
    "wa            1184\n",
    "australian    1122\n",
    "nsw            931\n",
    "election       903\n",
    "government     867\n",
    "sydney         865\n",
    "court          800\n",
    "day            774\n",
    "queensland     770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2017_18 = news_copy.loc[(news_copy['financial_year'] == '2017-18')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(subset2017_18.headline_text)\n",
    "news_dtm_subset2017_18 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "news_dtm_subset2017_18.index = subset2017_18.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_dtm_subset2017_18.sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "news_dtm_subset2017_18.sum().sort_values(ascending=False).head(15)\n",
    "says          1318\n",
    "trump         1236\n",
    "new           1208\n",
    "australia     1167\n",
    "police        1141\n",
    "australian     995\n",
    "man            916\n",
    "donald         701\n",
    "world          689\n",
    "north          675\n",
    "sydney         668\n",
    "court          643\n",
    "wa             602\n",
    "government     598\n",
    "nsw            597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2019_20 = news_copy.loc[(news_copy['financial_year'] == '2019-20')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(subset2019_20.headline_text)\n",
    "news_dtm_subset2019_20 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "news_dtm_subset2019_20.index = subset2019_20.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dtm_subset2019_20.sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_copy['financial_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subset2017_18.headline_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = nlp(''.join(str(subset2019_20.headline_text.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [x.text for x in tokens1.ents]\n",
    "Counter(items).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract people (real and fictional) using the PERSON type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/from-dataframe-to-named-entities-4cfaa7251fc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_list = []\n",
    "for ent in tokens1.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        person_list.append(ent.text)\n",
    "        \n",
    "person_counts = Counter(person_list).most_common(20)\n",
    "df_person = pd.DataFrame(person_counts, columns =['text', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person.plot.barh(x='text', y='count', title=\"Person\", figsize=(10,8)).invert_yaxis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NORP type recognizes nationalities, religious and political groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norp_list = []\n",
    "for ent in tokens1.ents:\n",
    "    if ent.label_ == 'NORP':\n",
    "        norp_list.append(ent.text)\n",
    "        \n",
    "norp_counts = Counter(norp_list).most_common(20)\n",
    "df_norp = pd.DataFrame(norp_counts, columns =['text', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norp.plot.barh(x='text', y='count', title=\"Nationalities, Religious, and Political Groups\", figsize=(10,8)).invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy NER - data too big, can't do here\n",
    "tokens = nlp(''.join(str(subset2017_18.headline_text.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(news_dtm_subset2017_18['headline_text'])\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(subset5.headline_text)\n",
    "dtm_subset5 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "dtm_subset5.index = subset5.index\n",
    "dtm_subset5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_subset5.sum().sort_values(ascending=False).head(10)\n",
    "#max.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_subset5.max().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(subset2004_05.headline_text)\n",
    "news_dtm_subset2004_05 = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "news_dtm_subset2004_05.index = subset2004_05.index\n",
    "news_dtm_subset2004_05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "# First Word tokenization\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "#Next find the roots of the word\n",
    "for w in nltk_tokens:\n",
    "       print \"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
